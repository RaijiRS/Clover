# ğŸŒ± Clover ğŸ€ â€” AI Assistant with Memory

**Clover** is a lightweight AI chatbot assistant powered by **FastAPI**, **React**, and **ChromaDB**. It supports file upload, webpage ingestion, contextual chat with memory, and live knowledge injection.

---

## ğŸ“¦ Features

- ğŸ’¬ **Stateful AI Chat** - Conversation history with session management
- ğŸ“š **Dynamic Knowledge Base** - Add raw text with source attribution
- ğŸŒ **Web Page Ingestion** - Load and index web pages automatically
- ğŸ“ **File Upload** - Upload and index `.txt` files
- ğŸ§  **Vector Search** - Semantic search using ChromaDB
- ğŸ¤– **Local LLM Support** - Powered by Ollama (Mistral or similar models)
- ğŸ”„ **Context-Aware Responses** - Uses vector search to provide relevant context

---

## ğŸ§° Tech Stack

- **Frontend:** React + Fetch API
- **Backend:** FastAPI (Python)
- **Vector DB:** ChromaDB (with persistence)
- **LLM Inference:** Ollama (Mistral or similar)
- **Embeddings:** Sentence Transformers (via ChromaDB)

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+ 
- Node.js and npm
- Ollama installed and running ([Download Ollama](https://ollama.ai))

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/clover.git
cd clover
```

### 2. Backend Setup

#### Create Virtual Environment

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**macOS/Linux:**
```bash
python -m venv venv
source venv/bin/activate
```

#### Install Dependencies

```bash
pip install -r requirements.txt
```

#### Start Backend API

```bash
uvicorn backend.main:app --reload
```

The API will be available at `http://localhost:8000`

#### Start Ollama LLM

In a **new terminal** (keep the backend running), activate the virtual environment and run:

```bash
# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate

# Run Ollama with Mistral (or your preferred model)
ollama run mistral
```

> **Note:** The code is optimized for Mistral, but you can use other Ollama models. Make sure the model is pulled first: `ollama pull mistral`

### 3. Frontend Setup

In a **new terminal**:

```bash
cd frontend
npm install
npm start
```

The frontend will be available at `http://localhost:3000` (or the port specified by React)

---

## ğŸ“¡ API Endpoints

### `POST /chat`
Send a message to the AI assistant.

**Request Body:**
```json
{
  "message": "Your question here",
  "conversation_id": "optional-uuid"
}
```

**Response:**
```json
{
  "response": "AI response text",
  "conversation_id": "uuid-string"
}
```

### `POST /add_knowledge`
Add raw text knowledge to the vector database.

**Request Body:**
```json
{
  "text": "Knowledge content here",
  "source": "Source name or URL"
}
```

### `POST /load_webpage`
Load and index a web page.

**Request Body:**
```json
{
  "url": "https://example.com"
}
```

### `POST /get_context`
Retrieve relevant context for a query.

**Request Body:**
```json
{
  "query": "Search query"
}
```

**Response:**
```json
{
  "context": "Retrieved context text"
}
```

### `POST /upload_text_file`
Upload a `.txt` file to be indexed.

**Request:** Multipart form data with `file` field

---

## ğŸ“ Project Structure

```
clover/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ ollama_handler.py    # Ollama integration
â”‚   â”œâ”€â”€ chroma_handler.py    # ChromaDB operations
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ frontend/                # React frontend
â”œâ”€â”€ chroma_storage/          # ChromaDB persistent storage
â”œâ”€â”€ venv/                    # Python virtual environment
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration

### Changing the LLM Model

Edit `backend/ollama_handler.py` to change the model name from `mistral` to your preferred Ollama model.

### ChromaDB Storage

The vector database is persisted in the `chroma_storage/` directory. To reset the knowledge base, delete this directory.

---

## ğŸ› Troubleshooting

### Backend won't start
- Ensure the virtual environment is activated
- Check that all dependencies are installed: `pip install -r requirements.txt`
- Verify Python version: `python --version` (should be 3.8+)

### Ollama connection issues
- Ensure Ollama is running: `ollama list`
- Verify the model is pulled: `ollama pull mistral`
- Check that Ollama is accessible at `http://localhost:11434`

### Frontend connection errors
- Ensure the backend is running on port 8000
- Check CORS settings in `backend/main.py`
- Verify the API URL in frontend configuration

---

## ğŸ“ License

[Add your license here]

---

## ğŸ¤ Contributing

[Add contribution guidelines here]

---

## ğŸ“§ Contact

[Add contact information here]
